{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Coding Masked Self-Attention in PyTroch!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Code a class that implements Masked Self Attention Mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model = 2,\n",
    "                 row_dim = 0,\n",
    "                 col_dim = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features = d_model, \n",
    "                             out_features = d_model, \n",
    "                             bias = False)\n",
    "        self.W_k = nn.Linear(in_features = d_model, \n",
    "                             out_features = d_model, \n",
    "                             bias = False)\n",
    "        self.W_v = nn.Linear(in_features = d_model, \n",
    "                             out_features = d_model, \n",
    "                             bias = False) \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method is where we actually calcuate the Masked Self-Attention values for each token. Just like before, we accept token encoding, but now also accpeting, Mask. etting default to None, we can use this to calcuate both MSA and SA both when needed.\n",
    "\n",
    "If mask is not None: we add the mask to the scaled similarities with masked_fill(). To understyand how we add, imaging mask is a matrice of Trues and False. True is replaced with -1e9 and False is 0. Then we add this to scaled similarities and futher they poass thrugh Softmax for percentage similarity and then passes through V to calcuates attention score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, token_encodings, mask = None):\n",
    "    q = self.W_q(token_encodings)\n",
    "    k = self.W_k(token_encodings)\n",
    "    v = self.W_v(token_encodings)\n",
    "\n",
    "    sims = torch.matmul(q,k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n",
    "    scaled_sims = sims / torch.tensor(k.size(self.col_dim) ** 0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_sims = scaled_sims.masked_fill(mask = mask, value = -1e9)\n",
    "\n",
    "    attention_percents = F.softmax(scaled_sims, dim = self.col_dim)\n",
    "    attention_scores = torch.matmul(attention_percents,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create a masked self attention object \n",
    "\n",
    "masked_self_attention = MaskedSelfAttention(d_model = 2,\n",
    "                                            row_dim = 0,\n",
    "                                            col_dim = 1)\n",
    "\n",
    "# create a mask so that we don'e use tokens that comes after the token of interest\n",
    "mask = torch.tril(torch.ones(3,3)) # Triangular Lower matrix of ones that is 1 below the diagonal and 0 above the diagonal.\n",
    "mask = mask == 0 # Invert the mask so that the lower triangle is False, and the upper triangle is True. \n",
    "#This basically means that we are masking out the tokens that come after the token of interest.\n",
    "# mask = mask == 0 is equivalent to mask = mask.bool() == False that is, we are converting the mask to a boolean tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_self_attention(encodings_matrix, mask) # this will return the attention scores for each token in the encodings matrix.\n",
    "# The attention scores are the weighted sum of the values of the tokens in the encodings matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
