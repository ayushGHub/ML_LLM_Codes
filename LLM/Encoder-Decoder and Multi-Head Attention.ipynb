{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Attention in PyTorch!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will code a class that is capable of all 3 types of Attention that we have studied, Self-Attention, Masked Self-Attention, and Encoder-Decoder Attention. We'll also code a few lines that will make Multi-Headed Attention work.\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- Code an Attention Class!!! This class will be able to perform Self-Attention, Masked-Self Attention, and Encoder-Decoder Attention.\n",
    "- Calculate Encoder-Decoder Attention Values!!! We'll then use the class that we created, Attention, to calculate Encoder-Decoder Attention values for some sample data.\n",
    "- Code Multi-Head Attention!!! We'll code Multi-Head Attention.\n",
    "- Calculate Mult-Head Attention!!!! Lastly, we calculate Multi-Head Attention values for some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    ## The only change from SelfAttention and attention is that\n",
    "    ## now we expect 3 sets of encodings to be passed in...\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Encoder-Decoder Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create matrices of token encodings...\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim=0,\n",
    "                      col_dim=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Mutli-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1, \n",
    "                 num_heads=1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "# Then we use a for loop to create a bunch of Attention instances (num heads) and store them in a ModuleList. \n",
    "# ModuleList is a special kind of list that tells PyTorch to keep track of the modules in the list. \n",
    "# This is important because PyTorch needs to know about all of the modules in the model in order to train it properly. \n",
    "# If we just used a regular Python list, PyTorch wouldn't know about the modules and wouldn't be able to train them.\n",
    "\n",
    "        ## create a bunch of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_model, row_dim, col_dim) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.col_dim = col_dim\n",
    "        \n",
    "    def forward(self, \n",
    "                encodings_for_q, \n",
    "                encodings_for_k,\n",
    "                encodings_for_v):\n",
    "\n",
    "        ## run the data through all of the attention heads\n",
    "        return torch.cat([head(encodings_for_q, \n",
    "                               encodings_for_k,\n",
    "                               encodings_for_v) \n",
    "                          for head in self.heads], dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcualte Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=2)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
